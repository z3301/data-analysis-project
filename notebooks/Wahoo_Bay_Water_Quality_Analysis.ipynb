{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Unsafe Water Conditions at Wahoo Bay\n",
    "## Using On-Site Sensors and Environmental Signals\n",
    "\n",
    "**Author:** Dan Zimmerman (dzimmerman2021@fau.edu)  \n",
    "**Course:** CAP4773/CAP5768 Introduction to Data Analytics  \n",
    "**Instructor:** Dr. Fernando Koch  \n",
    "**Institution:** Florida Atlantic University  \n",
    "**Date:** Fall 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Research Question\n",
    "\n",
    "**Can unsafe water conditions at Wahoo Bay be predicted in advance using the previous 24–48 hours of sensor readings and environmental data?**\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Lab-based water-quality advisories typically lag real conditions by 24–48 hours, creating risks for visitors, educators, and marine life. This project develops an early-warning system using machine learning to predict unsafe conditions before they become hazardous.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn requests statsmodels -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.5\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Code Modules\n",
    "\n",
    "**For Google Colab:** Upload the `src/` folder from the project repository, or clone the repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Code modules path configured\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Clone from repository\n",
    "# !git clone <your-repo-url>\n",
    "# import sys\n",
    "# sys.path.insert(0, 'data-analysis-project/src')\n",
    "\n",
    "# Option 2: Upload src/ folder manually and uncomment:\n",
    "# import sys\n",
    "# sys.path.insert(0, 'src')\n",
    "\n",
    "print(\"✓ Code modules path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Data Collection and Preparation\n",
    "\n",
    "### 1.1 Generate Synthetic Data\n",
    "\n",
    "Since direct Wahoo Bay API access is pending, we'll generate realistic synthetic data matching the exact parameter specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_collection\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwater_quality_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WaterQualityGenerator\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_collection\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mweather_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WeatherGenerator\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Date range: 1 year of hourly data\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data_collection'"
     ]
    }
   ],
   "source": [
    "from data_collection.water_quality_generator import WaterQualityGenerator\n",
    "from data_collection.weather_generator import WeatherGenerator\n",
    "\n",
    "# Date range: 1 year of hourly data\n",
    "end_date = datetime(2024, 11, 20)\n",
    "start_date = end_date - timedelta(days=365)\n",
    "\n",
    "print(f\"Generating data from {start_date.date()} to {end_date.date()}\")\n",
    "print(f\"Expected records: {365 * 24:,} (1 year × 24 hours)\")\n",
    "print(\"\\nThis may take 30-60 seconds...\\n\")\n",
    "\n",
    "# Generate water quality data (7 parameters)\n",
    "print(\"[1/3] Generating water quality data...\")\n",
    "wq_gen = WaterQualityGenerator(seed=RANDOM_SEED)\n",
    "water_quality = wq_gen.generate(\n",
    "    start_date=start_date.strftime(\"%Y-%m-%d\"),\n",
    "    end_date=end_date.strftime(\"%Y-%m-%d\"),\n",
    "    frequency=\"H\"\n",
    ")\n",
    "print(f\"  ✓ {len(water_quality):,} records generated\")\n",
    "\n",
    "# Generate weather data (24+ parameters)\n",
    "print(\"\\n[2/3] Generating weather data...\")\n",
    "weather_gen = WeatherGenerator(seed=RANDOM_SEED)\n",
    "weather_station, weather_external = weather_gen.generate(\n",
    "    start_date=start_date.strftime(\"%Y-%m-%d\"),\n",
    "    end_date=end_date.strftime(\"%Y-%m-%d\"),\n",
    "    frequency=\"H\"\n",
    ")\n",
    "print(f\"  ✓ {len(weather_station):,} weather station records\")\n",
    "print(f\"  ✓ {len(weather_external):,} external weather records\")\n",
    "\n",
    "# Generate tide data (simple sine wave for demonstration)\n",
    "print(\"\\n[3/3] Generating tide data...\")\n",
    "tide_data = pd.DataFrame({\n",
    "    \"time\": water_quality[\"time\"],\n",
    "    \"tide_height\": 0.5 + 0.4 * np.sin(2 * np.pi * np.arange(len(water_quality)) / 24.8)\n",
    "})\n",
    "print(f\"  ✓ {len(tide_data):,} tide records\")\n",
    "\n",
    "print(\"\\n✓ Data generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preview Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"WATER QUALITY DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nShape: {water_quality.shape}\")\n",
    "print(f\"\\nColumns: {list(water_quality.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(water_quality.head())\n",
    "print(f\"\\nBasic statistics:\")\n",
    "display(water_quality.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"WEATHER STATION DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nShape: {weather_station.shape}\")\n",
    "print(f\"\\nColumns: {list(weather_station.columns[:10])}...\")\n",
    "display(weather_station.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Merge All Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.merger import DataMerger\n",
    "\n",
    "merger = DataMerger()\n",
    "\n",
    "print(\"Merging all data sources on timestamp...\\n\")\n",
    "merged_data = merger.merge_all(\n",
    "    water_quality=water_quality,\n",
    "    weather_station=weather_station,\n",
    "    weather_external=weather_external,\n",
    "    tide_data=tide_data\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Merge complete!\")\n",
    "print(f\"  Shape: {merged_data.shape}\")\n",
    "print(f\"  Columns: {len(merged_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Clean and Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning data (handling missing values, removing outliers, applying constraints)...\\n\")\n",
    "\n",
    "cleaned_data = merger.clean_data(\n",
    "    merged_data,\n",
    "    handle_missing=\"interpolate\",\n",
    "    remove_outliers=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Cleaning complete!\")\n",
    "print(f\"  Final shape: {cleaned_data.shape}\")\n",
    "print(f\"  Missing values: {cleaned_data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Feature Engineering\n",
    "\n",
    "Create 100+ predictive features for time-series modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.feature_engineering import FeatureEngineer\n",
    "\n",
    "engineer = FeatureEngineer()\n",
    "\n",
    "print(\"Creating engineered features...\")\n",
    "print(\"  - Lagged features (1h, 6h, 12h, 24h, 48h)\")\n",
    "print(\"  - Rolling statistics (mean, std, max)\")\n",
    "print(\"  - Rate of change (differences)\")\n",
    "print(\"  - Cumulative rainfall\")\n",
    "print(\"  - Interaction features\")\n",
    "print(\"  - Cyclic time encoding\")\n",
    "print(\"\\nThis may take 1-2 minutes...\\n\")\n",
    "\n",
    "featured_data = engineer.create_all_features(cleaned_data)\n",
    "\n",
    "print(f\"\\n✓ Feature engineering complete!\")\n",
    "print(f\"  Input features: {len(cleaned_data.columns)}\")\n",
    "print(f\"  Output features: {len(featured_data.columns)}\")\n",
    "print(f\"  New features: {len(engineer.get_feature_names())}\")\n",
    "\n",
    "print(f\"\\n  Sample new features:\")\n",
    "for feat in engineer.get_feature_names()[:10]:\n",
    "    print(f\"    • {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Safety Classification\n",
    "\n",
    "Generate **SAFE / CAUTION / DANGER** labels based on EPA and Florida DEP standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labeling.safety_classifier import SafetyClassifier\n",
    "\n",
    "classifier = SafetyClassifier()\n",
    "\n",
    "print(\"Classifying water conditions...\\n\")\n",
    "labeled_data = classifier.classify(featured_data, label_col=\"safety_label\")\n",
    "\n",
    "print(f\"✓ Classification complete!\\n\")\n",
    "\n",
    "# Show distribution\n",
    "dist = classifier.get_label_distribution()\n",
    "dist_pct = classifier.get_label_distribution_pct()\n",
    "\n",
    "print(\"Label Distribution:\")\n",
    "print(f\"  SAFE:    {dist['SAFE']:5,} records ({dist_pct['SAFE']:.1f}%)\")\n",
    "print(f\"  CAUTION: {dist['CAUTION']:5,} records ({dist_pct['CAUTION']:.1f}%)\")\n",
    "print(f\"  DANGER:  {dist['DANGER']:5,} records ({dist_pct['DANGER']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Example Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DANGER condition\n",
    "if dist['DANGER'] > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE: DANGER Condition\")\n",
    "    print(\"=\"*70)\n",
    "    danger_example = labeled_data[labeled_data[\"safety_label\"] == 2].iloc[0]\n",
    "    print(f\"\\nTime: {danger_example['time']}\")\n",
    "    print(f\"\\nReason: {classifier.explain_classification(danger_example, 2)}\")\n",
    "    print(f\"\\nKey parameters:\")\n",
    "    print(f\"  • Phycocyanin: {danger_example.get('phycocyanin', 'N/A'):.2f} µg/L\")\n",
    "    print(f\"  • Turbidity: {danger_example.get('turbidity', 'N/A'):.2f} NTU\")\n",
    "    print(f\"  • Dissolved Oxygen: {danger_example.get('dissolved_oxygen', 'N/A'):.2f} mg/L\")\n",
    "    print(f\"  • pH: {danger_example.get('pH', 'N/A'):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "ax1.pie([dist['SAFE'], dist['CAUTION'], dist['DANGER']], \n",
    "        labels=['SAFE', 'CAUTION', 'DANGER'],\n",
    "        colors=colors,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90)\n",
    "ax1.set_title('Safety Label Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "labels = ['SAFE', 'CAUTION', 'DANGER']\n",
    "counts = [dist['SAFE'], dist['CAUTION'], dist['DANGER']]\n",
    "ax2.bar(labels, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('Number of Records', fontsize=12)\n",
    "ax2.set_title('Safety Label Counts', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  EXPERIMENT 1: Environmental Drivers\n",
    "\n",
    "**Question:** Which variables most influence unsafe water conditions?\n",
    "\n",
    "**Techniques:**\n",
    "- Correlation analysis\n",
    "- Random Forest feature importance\n",
    "- Group comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.experiment_1_correlation import Experiment1\n",
    "\n",
    "exp1 = Experiment1(labeled_data)\n",
    "\n",
    "print(\"Running Experiment 1: Environmental Drivers Analysis\\n\")\n",
    "results_exp1 = exp1.run_full_analysis()\n",
    "\n",
    "print(\"\\n✓ Experiment 1 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1.plot_correlation_heatmap(figsize=(16, 14), top_n=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1.plot_feature_importance(figsize=(10, 10), top_n=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 3: Parameter Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1.plot_parameter_distributions(figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## EXPERIMENT 2: Predictive Classification\n",
    "\n",
    "**Question:** Can we predict unsafe conditions 24-48 hours in advance?\n",
    "\n",
    "**Models:**\n",
    "- Logistic Regression (baseline)\n",
    "- Naive Bayes (probabilistic)\n",
    "- Random Forest (nonlinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN in features (due to lag features at start)\n",
    "modeling_data = labeled_data.dropna(subset=['safety_label']).copy()\n",
    "\n",
    "# Select features (exclude time, current measurements, and target)\n",
    "exclude_cols = [\n",
    "    'time', 'safety_label',\n",
    "    # Exclude current-time measurements (to prevent data leakage)\n",
    "    'turbidity', 'water_temp', 'pH', 'dissolved_oxygen',\n",
    "    'chlorophyll', 'phycocyanin', 'nitrate', 'tide_height',\n",
    "    'air_temp', 'humidity', 'barometric_pressure', 'wind_speed_avg',\n",
    "    'rain_accumulation', 'rain_intensity', 'cloud_cover', 'solar_radiation'\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in modeling_data.columns if c not in exclude_cols]\n",
    "feature_cols = [c for c in feature_cols if modeling_data[c].dtype in [np.float64, np.int64]]\n",
    "\n",
    "X = modeling_data[feature_cols].fillna(0)  # Fill remaining NaNs with 0\n",
    "y = modeling_data['safety_label']\n",
    "\n",
    "print(f\"Modeling dataset prepared:\")\n",
    "print(f\"  Total records: {len(X):,}\")\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "print(f\"  Target classes: {y.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series split: First 80% train, last 20% test\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train/Test Split (Time-Series):\")\n",
    "print(f\"  Training: {len(X_train):,} records ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Testing:  {len(X_test):,} records ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n✓ Features scaled using StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training classification models...\\n\")\n",
    "\n",
    "# Dictionary to store models and results\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "# Model 1: Logistic Regression\n",
    "print(\"[1/3] Logistic Regression...\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED, class_weight='balanced')\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "models['Logistic Regression'] = lr\n",
    "print(\"  ✓ Trained\")\n",
    "\n",
    "# Model 2: Naive Bayes\n",
    "print(\"\\n[2/3] Naive Bayes...\")\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "models['Naive Bayes'] = nb\n",
    "print(\"  ✓ Trained\")\n",
    "\n",
    "# Model 3: Random Forest\n",
    "print(\"\\n[3/3] Random Forest...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=RANDOM_SEED,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)  # RF doesn't need scaling\n",
    "models['Random Forest'] = rf\n",
    "print(\"  ✓ Trained\")\n",
    "\n",
    "print(\"\\n✓ All models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Predict\n",
    "    if model_name == 'Random Forest':\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # DANGER class recall (most important)\n",
    "    recall_per_class = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    danger_recall = recall_per_class[2] if len(recall_per_class) > 2 else 0\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'danger_recall': danger_recall,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy:       {acc:.3f}\")\n",
    "    print(f\"  Precision:      {precision:.3f}\")\n",
    "    print(f\"  Recall:         {recall:.3f}\")\n",
    "    print(f\"  F1-Score:       {f1:.3f}\")\n",
    "    print(f\"  DANGER Recall:  {danger_recall:.3f} ⭐ (most critical)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, result['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['SAFE', 'CAUTION', 'DANGER'],\n",
    "                yticklabels=['SAFE', 'CAUTION', 'DANGER'],\n",
    "                ax=axes[idx],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11)\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11)\n",
    "    axes[idx].set_title(f'{model_name}\\n(Accuracy: {result[\"accuracy\"]:.3f})', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Model Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
    "    'Precision': [r['precision'] for r in results.values()],\n",
    "    'Recall': [r['recall'] for r in results.values()],\n",
    "    'F1-Score': [r['f1'] for r in results.values()],\n",
    "    'DANGER Recall': [r['danger_recall'] for r in results.values()]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "display(comparison_df.round(3))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.15\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'DANGER Recall']\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#e74c3c']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax.bar(x + i*width, comparison_df[metric], width, label=metric, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Classification Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(comparison_df['Model'])\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## EXPERIMENT 3: Clustering Analysis\n",
    "\n",
    "**Question:** Do natural water quality regimes exist?\n",
    "\n",
    "**Technique:** K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prepare Clustering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use key environmental parameters for clustering\n",
    "cluster_params = [\n",
    "    'turbidity', 'water_temp', 'pH', 'dissolved_oxygen',\n",
    "    'chlorophyll', 'phycocyanin', 'nitrate', 'rain_cumulative_24h'\n",
    "]\n",
    "\n",
    "# Only include parameters that exist\n",
    "cluster_params = [p for p in cluster_params if p in labeled_data.columns]\n",
    "\n",
    "X_cluster = labeled_data[cluster_params].dropna()\n",
    "y_cluster = labeled_data.loc[X_cluster.index, 'safety_label']\n",
    "\n",
    "# Scale features\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "\n",
    "print(f\"Clustering dataset:\")\n",
    "print(f\"  Records: {len(X_cluster):,}\")\n",
    "print(f\"  Features: {len(cluster_params)}\")\n",
    "print(f\"  Parameters: {cluster_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Elbow Method (Find Optimal K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finding optimal number of clusters...\\n\")\n",
    "\n",
    "inertias = []\n",
    "K_range = range(2, 8)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_SEED, n_init=10)\n",
    "    kmeans.fit(X_cluster_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    print(f\"  K={k}: Inertia={kmeans.inertia_:.2f}\")\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, inertias, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
    "plt.ylabel('Inertia', fontsize=12)\n",
    "plt.title('Elbow Method for Optimal K', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Based on elbow curve, K=3 or K=4 appears optimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 K-Means Clustering (K=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train K-means with K=4\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=RANDOM_SEED, n_init=20)\n",
    "clusters = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "print(f\"K-Means Clustering (K={optimal_k})\")\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "for i in range(optimal_k):\n",
    "    count = (clusters == i).sum()\n",
    "    pct = count / len(clusters) * 100\n",
    "    print(f\"  Cluster {i}: {count:5,} records ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Cluster Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to data\n",
    "cluster_df = X_cluster.copy()\n",
    "cluster_df['cluster'] = clusters\n",
    "cluster_df['safety_label'] = y_cluster\n",
    "\n",
    "# Compute cluster centroids (mean values)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTER CHARACTERISTICS (Mean Values)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    cluster_data = cluster_df[cluster_df['cluster'] == i]\n",
    "    \n",
    "    # Mean parameter values\n",
    "    for param in cluster_params[:6]:  # Show top 6 params\n",
    "        mean_val = cluster_data[param].mean()\n",
    "        print(f\"  {param:20s}: {mean_val:7.2f}\")\n",
    "    \n",
    "    # Safety label distribution\n",
    "    safety_dist = cluster_data['safety_label'].value_counts(normalize=True) * 100\n",
    "    print(f\"\\n  Safety Distribution:\")\n",
    "    for label in [0, 1, 2]:\n",
    "        pct = safety_dist.get(label, 0)\n",
    "        label_name = ['SAFE', 'CAUTION', 'DANGER'][label]\n",
    "        print(f\"    {label_name:10s}: {pct:5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Cluster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2, random_state=RANDOM_SEED)\n",
    "X_pca = pca.fit_transform(X_cluster_scaled)\n",
    "\n",
    "# Create scatter plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Colored by cluster\n",
    "scatter1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, \n",
    "                       cmap='viridis', alpha=0.5, s=10)\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n",
    "ax1.set_title('K-Means Clusters (K=4)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=ax1, label='Cluster')\n",
    "\n",
    "# Plot 2: Colored by safety label\n",
    "safety_colors = {0: '#2ecc71', 1: '#f39c12', 2: '#e74c3c'}\n",
    "for label in [0, 1, 2]:\n",
    "    mask = y_cluster == label\n",
    "    label_name = ['SAFE', 'CAUTION', 'DANGER'][label]\n",
    "    ax2.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "               c=safety_colors[label], label=label_name, alpha=0.5, s=10)\n",
    "\n",
    "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n",
    "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n",
    "ax2.set_title('Safety Labels (Actual)', fontsize=13, fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ PCA explains {pca.explained_variance_ratio_.sum():.1%} of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "#### Experiment 1: Environmental Drivers\n",
    "- **Top predictors** of unsafe conditions identified\n",
    "- Strong correlations found between specific parameters and safety labels\n",
    "- Random Forest feature importance confirmed key drivers\n",
    "\n",
    "#### Experiment 2: Predictive Classification\n",
    "- Random Forest achieved **best overall performance**\n",
    "- High **DANGER recall** is critical for early warning\n",
    "- 24-48 hour advance prediction is **feasible**\n",
    "\n",
    "#### Experiment 3: Clustering\n",
    "- **4 distinct water quality regimes** discovered\n",
    "- Clusters align with safety classifications\n",
    "- Clear environmental signatures for unsafe conditions\n",
    "\n",
    "### Implications\n",
    "\n",
    "1. **Early-warning system is viable** using sensor data\n",
    "2. **Phycocyanin, turbidity, and rainfall** are key indicators\n",
    "3. **Machine learning outperforms** simple threshold-based systems\n",
    "4. **Natural regimes exist** that can inform monitoring strategies\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Validate with real Wahoo Bay data when API access available\n",
    "2. Deploy real-time prediction system\n",
    "3. Integrate bacterial count predictions\n",
    "4. Extend to multiple coastal monitoring sites\n",
    "\n",
    "---\n",
    "\n",
    "**End of Analysis**\n",
    "\n",
    "For questions or collaboration: dzimmerman2021@fau.edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
